package com.Spark.SparkStreamingEx;

public class LogAnalyzerStreaming {
	private static Function2<Long, Long, Long> SUM_REDUCER = (a, b) -> a + b;

	  private static class ValueComparator<K, V> implements Comparator<Tuple2<K, V>>, Serializable {
	    private Comparator<V> comparator;

	    public ValueComparator(Comparator<V> comparator) {
	      this.comparator = comparator;
	    }

	    @Override
	    public int compare(Tuple2<K, V> o1, Tuple2<K, V> o2) {
	      return comparator.compare(o1._2(), o2._2());
	    }
	  }

	  // Stats will be computed for the last window length of time.
	  private static final Duration WINDOW_LENGTH = new Duration(30 * 1000);
	  // Stats will be computed every slide interval time.
	  private static final Duration SLIDE_INTERVAL = new Duration(10 * 1000);

	  public static void main(String[] args) {
	    SparkConf conf = new SparkConf().setAppName("Log Analyzer Streaming");
	    JavaSparkContext sc = new JavaSparkContext(conf);

	    JavaStreamingContext jssc = new JavaStreamingContext(sc,
	        SLIDE_INTERVAL);  // This sets the update window to be every 10 seconds.

	    JavaReceiverInputDStream<String> logDataDStream =
	        jssc.socketTextStream("localhost", 9999);

	    // A DStream of Apache Access Logs.
	    JavaDStream<ApacheAccessLog> accessLogDStream =
	        logDataDStream.map(ApacheAccessLog::parseFromLogLine);

	    // Splits the accessLogDStream into a dstream of time windowed rdd's of apache access logs.
	    JavaDStream<ApacheAccessLog> windowDStream =
	        accessLogDStream.window(WINDOW_LENGTH, SLIDE_INTERVAL);

	    windowDStream.foreachRDD(accessLogs -> {
	      if (accessLogs.count() == 0) {
	        System.out.println("No access logs in this time interval");
	        return null;
	      }

	      // *** Note that this is code copied verbatim from LogAnalyzer.java.

	      // Calculate statistics based on the content size.
	      JavaRDD<Long> contentSizes =
	          accessLogs.map(ApacheAccessLog::getContentSize).cache();
	      System.out.println(String.format("Content Size Avg: %s, Min: %s, Max: %s",
	          contentSizes.reduce(SUM_REDUCER) / contentSizes.count(),
	          contentSizes.min(Comparator.naturalOrder()),
	          contentSizes.max(Comparator.naturalOrder())));

	      // Compute Response Code to Count.
	      List<Tuple2<Integer, Long>> responseCodeToCount =
	          accessLogs.mapToPair(log -> new Tuple2<>(log.getResponseCode(), 1L))
	              .reduceByKey(SUM_REDUCER)
	              .take(100);
	      System.out.println(String.format("Response code counts: %s", responseCodeToCount));

	      // Any IPAddress that has accessed the server more than 10 times.
	      List<String> ipAddresses =
	          accessLogs.mapToPair(log -> new Tuple2<>(log.getIpAddress(), 1L))
	              .reduceByKey(SUM_REDUCER)
	              .filter(tuple -> tuple._2() > 10)
	              .map(Tuple2::_1)
	              .take(100);
	      System.out.println(String.format("IPAddresses > 10 times: %s", ipAddresses));

	      // Top Endpoints.
	      List<Tuple2<String, Long>> topEndpoints = accessLogs
	          .mapToPair(log -> new Tuple2<>(log.getEndpoint(), 1L))
	          .reduceByKey(SUM_REDUCER)
	          .top(10, new ValueComparator<>(Comparator.<Long>naturalOrder()));
	      System.out.println(String.format("Top Endpoints: %s", topEndpoints));

	      return null;
	    });

	    // Start the streaming server.
	    jssc.start();              // Start the computation
	    jssc.awaitTermination();   // Wait for the computation to terminate
	  }
}
